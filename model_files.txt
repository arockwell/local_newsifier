"""Model imports and initialization."""

from local_newsifier.models.database.base import Base
from local_newsifier.models.database.article import ArticleDB
from local_newsifier.models.database.entity import EntityDB
from local_newsifier.models.database.analysis_result import AnalysisResultDB
from local_newsifier.models.entity_tracking import (
    CanonicalEntityDB,
    EntityMentionContextDB,
    EntityProfileDB,
    entity_mentions,
    entity_relationships
)

__all__ = [
    "Base",
    "ArticleDB",
    "EntityDB",
    "AnalysisResultDB",
    "CanonicalEntityDB",
    "EntityMentionContextDB",
    "EntityProfileDB",
    "entity_mentions",
    "entity_relationships"
]
"""Database models for the news analysis system."""

import enum
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from pydantic import BaseModel
from sqlalchemy.engine import Engine
from sqlalchemy.orm import sessionmaker

# Import directly from submodules to avoid circular imports
from local_newsifier.models.database.base import Base
from local_newsifier.models.database.article import ArticleDB
from local_newsifier.models.database.entity import EntityDB
from local_newsifier.models.database.analysis_result import AnalysisResultDB


# Pydantic Models
class ArticleBase(BaseModel):
    """Base Pydantic model for articles."""

    url: str
    title: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[datetime] = None
    content: Optional[str] = None
    status: Optional[str] = None


class ArticleCreate(ArticleBase):
    """Pydantic model for creating articles."""

    pass


class Article(ArticleBase):
    """Pydantic model for articles with relationships."""

    id: int
    scraped_at: datetime
    entities: List["Entity"] = []
    analysis_results: List["AnalysisResult"] = []

    class Config:
        """Pydantic config."""

        from_attributes = True


class EntityBase(BaseModel):
    """Base Pydantic model for entities."""

    text: str
    entity_type: str
    confidence: float


class EntityCreate(EntityBase):
    """Pydantic model for creating entities."""

    article_id: int


class Entity(EntityBase):
    """Pydantic model for entities with relationships."""

    id: int
    article_id: int

    class Config:
        """Pydantic config."""

        from_attributes = True


class AnalysisResultBase(BaseModel):
    """Base Pydantic model for analysis results."""

    analysis_type: str
    results: dict


class AnalysisResultCreate(AnalysisResultBase):
    """Pydantic model for creating analysis results."""

    article_id: int


class AnalysisResult(AnalysisResultBase):
    """Pydantic model for analysis results with relationships."""

    id: int
    article_id: int
    created_at: datetime

    class Config:
        """Pydantic config."""

        from_attributes = True


# Import initialization functions separately to avoid circular imports
from local_newsifier.models.database import init_db, get_session

# Re-export all models and functions
__all__ = [
    "Base",
    "ArticleDB",
    "EntityDB", 
    "AnalysisResultDB",
    "ArticleBase",
    "ArticleCreate",
    "Article",
    "EntityBase",
    "EntityCreate",
    "Entity",
    "AnalysisResultBase",
    "AnalysisResultCreate",
    "AnalysisResult",
    "init_db",
    "get_session"
]"""Entity tracking models for the news analysis system."""

from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from pydantic import BaseModel, Field
from sqlalchemy import (Column, DateTime, Float, ForeignKey, Integer, String, 
                        Table, Text, UniqueConstraint, JSON)
from sqlalchemy.orm import relationship

from local_newsifier.models.database.base import Base

# Association table for entity mentions
entity_mentions = Table(
    "entity_mentions",
    Base.metadata,
    Column("id", Integer, primary_key=True),
    Column("canonical_entity_id", Integer, ForeignKey("canonical_entities.id"), nullable=False),
    Column("entity_id", Integer, ForeignKey("entities.id"), nullable=False),
    Column("confidence", Float, default=1.0),
    Column("created_at", DateTime, default=lambda: datetime.now(timezone.utc)),
    UniqueConstraint("canonical_entity_id", "entity_id", name="uix_entity_mention"),
    extend_existing=True
)

# Association table for entity relationships
entity_relationships = Table(
    "entity_relationships",
    Base.metadata,
    Column("id", Integer, primary_key=True),
    Column("source_entity_id", Integer, ForeignKey("canonical_entities.id"), nullable=False),
    Column("target_entity_id", Integer, ForeignKey("canonical_entities.id"), nullable=False),
    Column("relationship_type", String, nullable=False),
    Column("confidence", Float, default=1.0),
    Column("evidence", Text),
    Column("created_at", DateTime, default=lambda: datetime.now(timezone.utc)),
    Column("updated_at", DateTime, default=lambda: datetime.now(timezone.utc), onupdate=lambda: datetime.now(timezone.utc)),
    UniqueConstraint("source_entity_id", "target_entity_id", "relationship_type", 
                    name="uix_entity_relationship"),
    extend_existing=True
)


class CanonicalEntityDB(Base):
    """Database model for canonical entities."""
    
    __tablename__ = "canonical_entities"
    
    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)
    entity_type = Column(String, nullable=False)  # e.g., "PERSON", "ORG", "GPE"
    description = Column(Text)
    entity_metadata = Column(JSON)  # Renamed from metadata to avoid SQLAlchemy reserved name
    first_seen = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    last_seen = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    
    # Define a unique constraint for name and entity_type
    __table_args__ = (
        UniqueConstraint('name', 'entity_type', name='uix_name_type'),
        {'extend_existing': True}
    )
    
    # Relationships
    mentions = relationship(
        "EntityDB", 
        secondary="entity_mentions", 
        backref="canonical_entities"
    )
    
    # Relationships with other entities
    related_to = relationship(
        "CanonicalEntityDB",
        secondary="entity_relationships",
        primaryjoin="CanonicalEntityDB.id==entity_relationships.c.source_entity_id",
        secondaryjoin="CanonicalEntityDB.id==entity_relationships.c.target_entity_id",
        backref="related_from"
    )


class EntityMentionContextDB(Base):
    """Database model for storing entity mention contexts."""
    
    __tablename__ = "entity_mention_contexts"
    
    id = Column(Integer, primary_key=True)
    entity_id = Column(Integer, ForeignKey("entities.id"), nullable=False)
    article_id = Column(Integer, ForeignKey("articles.id"), nullable=False)
    context_text = Column(Text, nullable=False)
    context_type = Column(String)  # e.g., "sentence", "paragraph"
    sentiment_score = Column(Float)
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    
    # Define a unique constraint
    __table_args__ = (
        UniqueConstraint('entity_id', 'article_id', name='uix_entity_article'),
        {'extend_existing': True}
    )
    
    # Relationships
    entity = relationship("EntityDB", backref="contexts")


class EntityProfileDB(Base):
    """Database model for entity profiles with aggregated information."""
    
    __tablename__ = "entity_profiles"
    
    id = Column(Integer, primary_key=True)
    canonical_entity_id = Column(Integer, ForeignKey("canonical_entities.id"), nullable=False)
    mention_count = Column(Integer, default=0)
    contexts = Column(JSON)  # Store sample contexts
    source_distribution = Column(JSON)  # Store count by source
    temporal_data = Column(JSON)  # Store mentions over time
    related_entities = Column(JSON)  # Store related entity counts
    related_topics = Column(JSON)  # Store related topic counts
    last_updated = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    
    # Define table args
    __table_args__ = {'extend_existing': True}
    
    # Relationships
    canonical_entity = relationship("CanonicalEntityDB", backref="profile")


# Pydantic Models

class CanonicalEntityBase(BaseModel):
    """Base Pydantic model for canonical entities."""
    
    name: str
    entity_type: str
    description: Optional[str] = None
    entity_metadata: Optional[Dict[str, Any]] = None  # Renamed from metadata


class CanonicalEntityCreate(CanonicalEntityBase):
    """Pydantic model for creating canonical entities."""
    
    pass


class CanonicalEntity(CanonicalEntityBase):
    """Pydantic model for canonical entities with relationships."""
    
    id: int
    first_seen: datetime
    last_seen: datetime
    mention_count: Optional[int] = None
    
    class Config:
        """Pydantic config."""
        
        from_attributes = True


class EntityMentionContextBase(BaseModel):
    """Base Pydantic model for entity mention contexts."""
    
    entity_id: int
    article_id: int
    context_text: str
    context_type: Optional[str] = "sentence"
    sentiment_score: Optional[float] = None


class EntityMentionContextCreate(EntityMentionContextBase):
    """Pydantic model for creating entity mention contexts."""
    
    pass


class EntityMentionContext(EntityMentionContextBase):
    """Pydantic model for entity mention contexts with relationships."""
    
    id: int
    created_at: datetime
    
    class Config:
        """Pydantic config."""
        
        from_attributes = True


class EntityProfileBase(BaseModel):
    """Base Pydantic model for entity profiles."""
    
    canonical_entity_id: int
    mention_count: int = 0
    contexts: Optional[List[str]] = None
    source_distribution: Optional[Dict[str, int]] = None
    temporal_data: Optional[Dict[str, int]] = None
    related_entities: Optional[Dict[str, int]] = None
    related_topics: Optional[Dict[str, int]] = None


class EntityProfileCreate(EntityProfileBase):
    """Pydantic model for creating entity profiles."""
    
    pass


class EntityProfile(EntityProfileBase):
    """Pydantic model for entity profiles with relationships."""
    
    id: int
    last_updated: datetime
    canonical_entity: Optional[CanonicalEntity] = None
    
    class Config:
        """Pydantic config."""
        
        from_attributes = True


class EntityRelationshipBase(BaseModel):
    """Base Pydantic model for entity relationships."""
    
    source_entity_id: int
    target_entity_id: int
    relationship_type: str
    confidence: float = 1.0
    evidence: Optional[str] = None


class EntityRelationshipCreate(EntityRelationshipBase):
    """Pydantic model for creating entity relationships."""
    
    pass


class EntityRelationship(EntityRelationshipBase):
    """Pydantic model for entity relationships with additional info."""
    
    id: int
    created_at: datetime
    updated_at: datetime
    source_entity: Optional[CanonicalEntity] = None
    target_entity: Optional[CanonicalEntity] = None
    
    class Config:
        """Pydantic config."""
        
        from_attributes = True"""Sentiment analysis models for the news analysis system."""

from datetime import datetime, timezone
from typing import Dict, List, Optional, Any

from pydantic import BaseModel, Field
from sqlalchemy import (Column, DateTime, Float, ForeignKey, Integer, String, 
                       Table, Text, UniqueConstraint, JSON)
from sqlalchemy.orm import relationship

from .database import Base, ArticleDB


class SentimentAnalysisDB(Base):
    """Database model for article sentiment analysis."""
    
    __tablename__ = "sentiment_analyses"
    
    id = Column(Integer, primary_key=True)
    article_id = Column(Integer, ForeignKey("articles.id"), nullable=False)
    document_sentiment = Column(Float)  # Overall document sentiment score
    document_magnitude = Column(Float)  # Overall document magnitude
    entity_sentiments = Column(JSON)  # Sentiment scores by entity
    topic_sentiments = Column(JSON)  # Sentiment scores by topic
    analyzed_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    
    # Define a unique constraint
    __table_args__ = (
        UniqueConstraint('article_id', name='uix_sentiment_article'),
    )
    
    # Relationships
    article = relationship("ArticleDB", backref="sentiment_analysis")


class OpinionTrendDB(Base):
    """Database model for tracking sentiment trends over time."""
    
    __tablename__ = "opinion_trends"
    
    id = Column(Integer, primary_key=True)
    topic = Column(String, nullable=False)
    period = Column(String, nullable=False)  # e.g. "2023-01-01"
    period_type = Column(String, nullable=False)  # e.g. "day", "week", "month"
    avg_sentiment = Column(Float)
    sentiment_count = Column(Integer)
    sentiment_distribution = Column(JSON)  # Distribution of sentiment scores
    sources = Column(JSON)  # Sources contributing to this trend
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    
    # Define a unique constraint
    __table_args__ = (
        UniqueConstraint('topic', 'period', 'period_type', name='uix_topic_period'),
    )


class SentimentShiftDB(Base):
    """Database model for tracking significant sentiment shifts."""
    
    __tablename__ = "sentiment_shifts"
    
    id = Column(Integer, primary_key=True)
    topic = Column(String, nullable=False)
    start_period = Column(String, nullable=False)
    end_period = Column(String, nullable=False)
    period_type = Column(String, nullable=False)  # e.g. "day", "week", "month"
    start_sentiment = Column(Float)
    end_sentiment = Column(Float)
    shift_magnitude = Column(Float)  # Absolute change
    shift_percentage = Column(Float)  # Percentage change
    supporting_article_ids = Column(JSON)  # List of article IDs supporting this shift
    detected_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))


# Pydantic Models

class SentimentAnalysisBase(BaseModel):
    """Base Pydantic model for sentiment analysis."""
    
    article_id: int
    document_sentiment: float
    document_magnitude: float
    entity_sentiments: Optional[Dict[str, float]] = None
    topic_sentiments: Optional[Dict[str, float]] = None


class SentimentAnalysisCreate(SentimentAnalysisBase):
    """Pydantic model for creating sentiment analysis."""
    
    pass


class SentimentAnalysis(SentimentAnalysisBase):
    """Pydantic model for sentiment analysis with relationships."""
    
    id: int
    analyzed_at: datetime
    
    class Config:
        """Pydantic config."""
        
        from_attributes = True


class OpinionTrendBase(BaseModel):
    """Base Pydantic model for opinion trends."""
    
    topic: str
    period: str
    period_type: str
    avg_sentiment: float
    sentiment_count: int
    sentiment_distribution: Optional[Dict[str, int]] = None
    sources: Optional[Dict[str, int]] = None


class OpinionTrendCreate(OpinionTrendBase):
    """Pydantic model for creating opinion trends."""
    
    pass


class OpinionTrend(OpinionTrendBase):
    """Pydantic model for opinion trends with relationships."""
    
    id: int
    created_at: datetime
    
    class Config:
        """Pydantic config."""
        
        from_attributes = True


class SentimentShiftBase(BaseModel):
    """Base Pydantic model for sentiment shifts."""
    
    topic: str
    start_period: str
    end_period: str
    period_type: str
    start_sentiment: float
    end_sentiment: float
    shift_magnitude: float
    shift_percentage: float
    supporting_article_ids: List[int] = []


class SentimentShiftCreate(SentimentShiftBase):
    """Pydantic model for creating sentiment shifts."""
    
    pass


class SentimentShift(SentimentShiftBase):
    """Pydantic model for sentiment shifts with relationships."""
    
    id: int
    detected_at: datetime
    
    class Config:
        """Pydantic config."""
        
        from_attributes = True


class SentimentVisualizationData(BaseModel):
    """Pydantic model for sentiment visualization data."""
    
    topic: str
    time_periods: List[str]
    sentiment_values: List[float]
    confidence_intervals: Optional[List[Dict[str, float]]] = None
    article_counts: List[int]
    metadata: Optional[Dict[str, Any]] = Nonefrom datetime import datetime, timezone
from enum import Enum
from typing import Dict, List, Optional
from uuid import UUID, uuid4

from pydantic import BaseModel, ConfigDict, Field


class AnalysisStatus(str, Enum):
    """Status of the analysis pipeline."""

    INITIALIZED = "INITIALIZED"
    SCRAPING = "SCRAPING"
    SCRAPE_SUCCEEDED = "SCRAPE_SUCCEEDED"
    SCRAPE_FAILED_NETWORK = "SCRAPE_FAILED_NETWORK"
    SCRAPE_FAILED_PARSING = "SCRAPE_FAILED_PARSING"
    ANALYZING = "ANALYZING"
    ANALYSIS_SUCCEEDED = "ANALYSIS_SUCCEEDED"
    ANALYSIS_FAILED = "ANALYSIS_FAILED"
    SAVING = "SAVING"
    SAVE_SUCCEEDED = "SAVE_SUCCEEDED"
    SAVE_FAILED = "SAVE_FAILED"
    COMPLETED_SUCCESS = "COMPLETED_SUCCESS"
    COMPLETED_WITH_ERRORS = "COMPLETED_WITH_ERRORS"


class ErrorDetails(BaseModel):
    """Details about an error that occurred during processing."""

    task: str
    type: str
    message: str
    traceback_snippet: Optional[str] = None


class NewsAnalysisState(BaseModel):
    """State model for the news analysis pipeline."""

    run_id: UUID = Field(default_factory=uuid4)
    target_url: str
    scraped_at: Optional[datetime] = None
    scraped_text: Optional[str] = None
    analyzed_at: Optional[datetime] = None
    analysis_config: Dict[str, List[str]] = Field(
        default_factory=lambda: {"entity_types": ["PERSON", "ORG", "GPE"]}
    )
    analysis_results: Optional[Dict] = None
    saved_at: Optional[datetime] = None
    save_path: Optional[str] = None
    status: AnalysisStatus = Field(default=AnalysisStatus.INITIALIZED)
    error_details: Optional[ErrorDetails] = None
    retry_count: int = Field(default=0)
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    run_logs: List[str] = Field(default_factory=list)

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "run_id": "123e4567-e89b-12d3-a456-426614174000",
                "target_url": "https://example.com/news/article",
                "status": "INITIALIZED",
                "analysis_config": {"entity_types": ["PERSON", "ORG", "GPE"]},
                "run_logs": [],
            }
        }
    )

    def touch(self) -> None:
        """Update the last_updated timestamp."""
        self.last_updated = datetime.now(timezone.utc)

    def add_log(self, message: str) -> None:
        """Add a log message with timestamp."""
        timestamp = datetime.now(timezone.utc).isoformat()
        self.run_logs.append(f"[{timestamp}] {message}")
        self.touch()

    def set_error(self, task: str, error: Exception) -> None:
        """Set error details and update status."""
        self.error_details = ErrorDetails(
            task=task,
            type=error.__class__.__name__,
            message=str(error),
            traceback_snippet=str(error.__traceback__),
        )
        self.touch()
"""Models for trend detection and analysis in local news articles."""

from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional, Union
from uuid import UUID, uuid4

from pydantic import BaseModel, ConfigDict, Field


class TrendType(str, Enum):
    """Types of trends that can be detected in news articles."""

    EMERGING_TOPIC = "EMERGING_TOPIC"
    FREQUENCY_SPIKE = "FREQUENCY_SPIKE"
    NOVEL_ENTITY = "NOVEL_ENTITY"
    SUSTAINED_COVERAGE = "SUSTAINED_COVERAGE"
    ANOMALOUS_PATTERN = "ANOMALOUS_PATTERN"


class TrendStatus(str, Enum):
    """Status of a detected trend."""

    POTENTIAL = "POTENTIAL"
    CONFIRMED = "CONFIRMED"
    DECLINING = "DECLINING"
    EXPIRED = "EXPIRED"


class TopicFrequency(BaseModel):
    """Model representing frequency information for a topic over time."""

    topic: str
    frequencies: Dict[str, int]  # date string -> count
    entity_type: Optional[str] = None
    total_mentions: int = 0

    def add_occurrence(self, date: Union[datetime, str], count: int = 1) -> None:
        """
        Add an occurrence of the topic on a specific date.

        Args:
            date: The date of the occurrence
            count: Number of occurrences to add
        """
        date_str = date.isoformat().split("T")[0] if isinstance(date, datetime) else date
        if date_str in self.frequencies:
            self.frequencies[date_str] += count
        else:
            self.frequencies[date_str] = count
        self.total_mentions += count


class TrendEvidenceItem(BaseModel):
    """A single piece of evidence supporting a trend."""

    article_id: Optional[int] = None
    article_url: str
    article_title: Optional[str] = None
    published_at: datetime
    evidence_text: str
    relevance_score: float = 1.0


class TrendEntity(BaseModel):
    """An entity associated with a trend."""

    text: str
    entity_type: str
    frequency: int = 1
    relevance_score: float = 1.0


class TrendAnalysis(BaseModel):
    """Model representing a detected trend in news articles."""

    trend_id: UUID = Field(default_factory=uuid4)
    trend_type: TrendType
    name: str
    description: str
    status: TrendStatus = TrendStatus.POTENTIAL
    confidence_score: float
    start_date: datetime
    end_date: Optional[datetime] = None
    last_updated: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    entities: List[TrendEntity] = Field(default_factory=list)
    evidence: List[TrendEvidenceItem] = Field(default_factory=list)
    frequency_data: Dict[str, int] = Field(default_factory=dict)  # date -> count
    statistical_significance: Optional[float] = None
    tags: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "trend_id": "123e4567-e89b-12d3-a456-426614174000",
                "trend_type": "EMERGING_TOPIC",
                "name": "Downtown Development Controversy",
                "description": "Increasing discussion of the new downtown development project",
                "status": "CONFIRMED",
                "confidence_score": 0.85,
                "start_date": "2023-01-15T00:00:00Z",
                "evidence": [
                    {
                        "article_url": "https://example.com/news/article1",
                        "published_at": "2023-01-15T14:30:00Z",
                        "evidence_text": "Controversy surrounds the downtown development...",
                    }
                ],
            }
        }
    )

    def add_evidence(self, item: TrendEvidenceItem) -> None:
        """
        Add a piece of evidence to the trend.

        Args:
            item: Evidence item to add
        """
        self.evidence.append(item)
        date_str = item.published_at.date().isoformat()
        
        # Update frequency data
        if date_str in self.frequency_data:
            self.frequency_data[date_str] += 1
        else:
            self.frequency_data[date_str] = 1
            
        # Update the last updated timestamp
        self.last_updated = datetime.now(timezone.utc)

    def add_entity(self, entity: TrendEntity) -> None:
        """
        Add or update an entity related to this trend.

        Args:
            entity: Entity to add or update
        """
        # Check if entity already exists
        for existing in self.entities:
            if existing.text == entity.text and existing.entity_type == entity.entity_type:
                existing.frequency += entity.frequency
                existing.relevance_score = max(existing.relevance_score, entity.relevance_score)
                return
                
        # Add new entity
        self.entities.append(entity)
        self.last_updated = datetime.now(timezone.utc)


class TimeFrame(str, Enum):
    """Time frames for trend analysis."""

    DAY = "DAY"
    WEEK = "WEEK"
    MONTH = "MONTH"
    QUARTER = "QUARTER"
    YEAR = "YEAR"


class TrendAnalysisConfig(BaseModel):
    """Configuration for trend analysis."""

    time_frame: TimeFrame = TimeFrame.WEEK
    min_articles: int = 3
    min_confidence: float = 0.6
    entity_types: List[str] = Field(default_factory=lambda: ["PERSON", "ORG", "GPE"])
    significance_threshold: float = 1.5  # Z-score threshold
    topic_limit: int = 20  # Max number of topics to track
    include_historical: bool = True
    lookback_periods: int = 4  # How many periods to look back"""Database models package."""

from sqlalchemy import create_engine
from sqlalchemy.engine import Engine
from sqlalchemy.orm import sessionmaker

from local_newsifier.models.database.base import Base
from local_newsifier.models.database.article import ArticleDB
from local_newsifier.models.database.entity import EntityDB
from local_newsifier.models.database.analysis_result import AnalysisResultDB

# Import Pydantic models from a separate file to avoid circular imports
from local_newsifier.models.database.pydantic_models import (
    Article, ArticleCreate, ArticleBase,
    Entity, EntityCreate, EntityBase,
    AnalysisResult, AnalysisResultCreate, AnalysisResultBase
)

# Re-export all models
__all__ = [
    "Base",
    "ArticleDB", "Article", "ArticleCreate", "ArticleBase",
    "EntityDB", "Entity", "EntityCreate", "EntityBase",
    "AnalysisResultDB", "AnalysisResult", "AnalysisResultCreate", "AnalysisResultBase"
]

def init_db(db_url: str) -> Engine:
    """Initialize the database and create tables.

    Args:
        db_url: Database connection URL

    Returns:
        SQLAlchemy engine instance
    """
    engine = create_engine(db_url)
    Base.metadata.create_all(engine)
    return engine

def get_session(engine: Engine) -> sessionmaker:
    """Get a session factory for the database.

    Args:
        engine: SQLAlchemy engine instance

    Returns:
        Session factory
    """
    return sessionmaker(bind=engine)"""Analysis result database model for the news analysis system."""

from datetime import datetime, timezone
from typing import Dict, Any, Optional

from sqlalchemy import Column, DateTime, ForeignKey, Index, Integer, JSON, String
from sqlalchemy.orm import relationship

from local_newsifier.models.database.base import Base


class AnalysisResultDB(Base):
    """Database model for analysis results."""
    
    __tablename__ = "analysis_results"
    
    # Foreign key to article
    article_id = Column(Integer, ForeignKey("articles.id"), nullable=False)
    
    # Analysis fields
    analysis_type = Column(String, nullable=False)  # e.g., "NER", "sentiment"
    results = Column(JSON, nullable=False)
    
    # Set created_at for backward compatibility (already included from Base)
    
    # Relationships
    article = relationship("ArticleDB", back_populates="analysis_results")
    
    # Indexes
    __table_args__ = (
        Index("ix_analysis_results_type", "analysis_type"),
        Index("ix_analysis_results_article_type", "article_id", "analysis_type"),
    )
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return f"<AnalysisResultDB(id={self.id}, type='{self.analysis_type}')>"
    
    @classmethod
    def from_analysis_result_create(cls, result_data: dict) -> "AnalysisResultDB":
        """Create an AnalysisResultDB instance from analysis result data."""
        return cls(**result_data)"""Article database model for the news analysis system."""

from datetime import datetime, timezone
from typing import List, Optional, TYPE_CHECKING

from pydantic import BaseModel
from sqlalchemy import Column, DateTime, Enum, ForeignKey, Index, String, Text
from sqlalchemy.orm import relationship

from local_newsifier.models.database.base import Base
from local_newsifier.models.state import AnalysisStatus

if TYPE_CHECKING:
    from .entity import Entity
    from .analysis_result import AnalysisResult


class ArticleBase(BaseModel):
    """Base Pydantic model for articles."""

    url: str
    title: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[datetime] = None
    content: Optional[str] = None
    status: Optional[str] = None


class ArticleCreate(ArticleBase):
    """Pydantic model for creating articles."""

    pass


class Article(ArticleBase):
    """Pydantic model for articles with relationships."""

    id: int
    scraped_at: datetime
    entities: List["Entity"] = []
    analysis_results: List["AnalysisResult"] = []

    class Config:
        """Pydantic config."""

        from_attributes = True


class ArticleDB(Base):
    """Database model for news articles."""
    
    __tablename__ = "articles"
    
    # Primary fields
    url = Column(String, unique=True, nullable=False, index=True)
    title = Column(String)
    source = Column(String, index=True)  # Keep 'source' name for backward compatibility
    published_at = Column(DateTime)
    
    # Content fields
    content = Column(Text)  # Keep 'content' name for backward compatibility
    scraped_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    
    # Status fields - allow string values for backward compatibility
    status = Column(
        String,
        default=AnalysisStatus.INITIALIZED.value,
        nullable=False
    )
    
    # Relationships
    entities = relationship(
        "EntityDB", 
        back_populates="article",
        cascade="all, delete-orphan"
    )
    analysis_results = relationship(
        "AnalysisResultDB", 
        back_populates="article",
        cascade="all, delete-orphan"
    )
    
    # Indexes
    __table_args__ = (
        Index("ix_articles_status", "status"),
        Index("ix_articles_scraped_at", "scraped_at"),
    )
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return f"<ArticleDB(id={self.id}, url='{self.url}', status='{self.status}')>"
        
    @classmethod
    def from_article_create(cls, article_data: dict) -> "ArticleDB":
        """Create an ArticleDB instance from article data."""
        return cls(**article_data)"""Base database model with common fields for all models."""

from datetime import datetime, timezone
from typing import Any, Dict

from sqlalchemy import Column, DateTime, Integer, MetaData
from sqlalchemy.ext.declarative import declared_attr
from sqlalchemy.orm import declarative_base


class BaseModel:
    """Base SQLAlchemy model with common fields."""
    
    @declared_attr
    def __tablename__(cls) -> str:
        """Create tablename from class name."""
        return cls.__name__.lower()
    
    id = Column(Integer, primary_key=True)
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    updated_at = Column(
        DateTime, 
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc)
    )

    def model_dump(self) -> Dict[str, Any]:
        """Convert model to dictionary for Pydantic compatibility."""
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}


# Create a shared metadata instance
metadata = MetaData()

# Create a base class for all models
Base = declarative_base(cls=BaseModel, metadata=metadata)"""Entity database model for the news analysis system."""

from datetime import datetime, timezone
from typing import Optional

from sqlalchemy import Column, Float, ForeignKey, Index, Integer, String, Text
from sqlalchemy.orm import relationship

from local_newsifier.models.database.base import Base


class EntityDB(Base):
    """Database model for named entities found in articles."""
    
    __tablename__ = "entities"
    
    # Foreign key to article
    article_id = Column(Integer, ForeignKey("articles.id"), nullable=False)
    
    # Entity fields
    text = Column(String, nullable=False)
    entity_type = Column(String, nullable=False)  # PERSON, ORG, GPE, etc.
    confidence = Column(Float, default=1.0)
    
    # Add sentence_context field for storing context
    sentence_context = Column(Text)  # The sentence where the entity was found
    
    # Set created_at for backward compatibility (already included from Base)
    
    # Relationships
    article = relationship("ArticleDB", back_populates="entities")
    
    # Indexes
    __table_args__ = (
        Index("ix_entities_text", "text"),
        Index("ix_entities_type", "entity_type"),
        Index("ix_entities_article_type", "article_id", "entity_type"),
    )
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return f"<EntityDB(id={self.id}, text='{self.text}', type='{self.entity_type}')>"
    
    @classmethod
    def from_entity_create(cls, entity_data: dict) -> "EntityDB":
        """Create an EntityDB instance from entity data."""
        return cls(**entity_data)"""Pydantic models for the database."""

from datetime import datetime
from typing import List, Optional

from pydantic import BaseModel


class ArticleBase(BaseModel):
    """Base Pydantic model for articles."""

    url: str
    title: Optional[str] = None
    source: Optional[str] = None
    published_at: Optional[datetime] = None
    content: Optional[str] = None
    status: Optional[str] = None


class ArticleCreate(ArticleBase):
    """Pydantic model for creating articles."""

    pass


class Article(ArticleBase):
    """Pydantic model for articles with relationships."""

    id: int
    scraped_at: datetime
    entities: List["Entity"] = []
    analysis_results: List["AnalysisResult"] = []

    class Config:
        """Pydantic config."""

        from_attributes = True


class EntityBase(BaseModel):
    """Base Pydantic model for entities."""

    text: str
    entity_type: str
    confidence: float


class EntityCreate(EntityBase):
    """Pydantic model for creating entities."""

    article_id: int


class Entity(EntityBase):
    """Pydantic model for entities with relationships."""

    id: int
    article_id: int

    class Config:
        """Pydantic config."""

        from_attributes = True


class AnalysisResultBase(BaseModel):
    """Base Pydantic model for analysis results."""

    analysis_type: str
    results: dict


class AnalysisResultCreate(AnalysisResultBase):
    """Pydantic model for creating analysis results."""

    article_id: int


class AnalysisResult(AnalysisResultBase):
    """Pydantic model for analysis results with relationships."""

    id: int
    article_id: int
    created_at: datetime

    class Config:
        """Pydantic config."""

        from_attributes = True """Configuration package for the Local Newsifier application."""

from local_newsifier.config.settings import Settings, get_settings, get_cursor_db_name

# Import after settings to avoid circular imports
from local_newsifier.config.database import (
    DatabaseSettings,
    get_database,
    get_database_settings,
    get_db_session,
)

__all__ = [
    "DatabaseSettings",
    "Settings",
    "get_cursor_db_name",
    "get_database",
    "get_database_settings",
    "get_db_session",
    "get_settings",
]"""Database configuration and connection management."""

import os
import uuid
from pathlib import Path
from typing import Optional, Any

from pydantic_settings import BaseSettings
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from local_newsifier.models.database import Base, init_db


def get_cursor_db_name() -> str:
    """Get the database name for the current cursor instance.
    
    Returns:
        Database name with cursor ID
    """
    cursor_id = os.environ.get("CURSOR_DB_ID")
    if not cursor_id:
        cursor_id = str(uuid.uuid4())[:8]
        os.environ["CURSOR_DB_ID"] = cursor_id
    return f"local_newsifier_{cursor_id}"


class DatabaseSettings(BaseSettings):
    """Database configuration settings."""
    
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_HOST: str = "localhost"
    POSTGRES_PORT: str = "5432"
    POSTGRES_DB: str = get_cursor_db_name()
    
    def get_database_url(self) -> str:
        """Get the database URL."""
        # Always use PostgreSQL for production
        return f"postgresql://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_HOST}:{self.POSTGRES_PORT}/{self.POSTGRES_DB}"

    model_config = {
        "env_prefix": "",
        "env_file": ".env",
        "env_file_encoding": "utf-8",
        "case_sensitive": True,
        "extra": "allow",  # Allow extra attributes
    }


def get_database(env_file: Optional[str] = None) -> Any:
    """Get a database engine instance.
    
    Args:
        env_file: Optional path to .env file to load settings from
        
    Returns:
        SQLAlchemy engine instance
    """
    settings = DatabaseSettings(_env_file=env_file if env_file else None)
    return init_db(settings.get_database_url())


def get_db_session(env_file: Optional[str] = None) -> sessionmaker:
    """Get a database session factory.
    
    Args:
        env_file: Optional path to .env file to load settings from
        
    Returns:
        SQLAlchemy session factory
    """
    engine = get_database(env_file)
    return sessionmaker(bind=engine)


def get_database_settings(env_file: Optional[str] = None) -> DatabaseSettings:
    """Get database settings.
    
    Args:
        env_file: Optional path to .env file to load settings from
        
    Returns:
        DatabaseSettings instance
    """
    return DatabaseSettings(_env_file=env_file if env_file else None)"""Application settings configuration."""

import os
import uuid
from pathlib import Path
from typing import Dict, List, Optional, Union

from pydantic import Field
from pydantic_settings import BaseSettings


def get_cursor_db_name() -> str:
    """Get a cursor-specific database name.
    
    Returns:
        Database name with cursor ID
    """
    cursor_id = os.environ.get("CURSOR_DB_ID")
    if not cursor_id:
        cursor_id = str(uuid.uuid4())[:8]
        os.environ["CURSOR_DB_ID"] = cursor_id
    return f"local_newsifier_{cursor_id}"


class Settings(BaseSettings):
    """Application settings using Pydantic BaseSettings."""
    
    # Database settings
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_HOST: str = "localhost"
    POSTGRES_PORT: str = "5432"
    POSTGRES_DB: str = Field(default_factory=get_cursor_db_name)
    DB_POOL_SIZE: int = 5
    DB_MAX_OVERFLOW: int = 10
    
    # Directory settings
    OUTPUT_DIR: Path = Field(default_factory=lambda: Path("output"))
    CACHE_DIR: Path = Field(default_factory=lambda: Path("cache"))
    TEMP_DIR: Path = Field(default_factory=lambda: Path("temp"))
    
    # Logging settings
    LOG_LEVEL: str = "INFO"
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    LOG_FILE: Optional[Path] = None
    
    # Scraping settings
    USER_AGENT: str = "Local-Newsifier/1.0"
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    RETRY_DELAY: int = 5
    
    # NER analysis settings
    NER_MODEL: str = "en_core_web_lg"
    ENTITY_TYPES: List[str] = Field(default_factory=lambda: ["PERSON", "ORG", "GPE"])
    
    def get_database_url(self) -> str:
        """Get the database URL based on environment."""
        # Always use PostgreSQL
        return f"postgresql://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_HOST}:{self.POSTGRES_PORT}/{self.POSTGRES_DB}"
    
    def create_directories(self) -> None:
        """Create necessary directories if they don't exist."""
        self.OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
        self.CACHE_DIR.mkdir(exist_ok=True, parents=True)
        self.TEMP_DIR.mkdir(exist_ok=True, parents=True)
        
        if self.LOG_FILE:
            self.LOG_FILE.parent.mkdir(exist_ok=True, parents=True)
    
    model_config = {
        "env_prefix": "",
        "env_file": ".env",
        "env_file_encoding": "utf-8",
        "case_sensitive": True,
    }


def get_settings() -> Settings:
    """Get application settings singleton.
    
    Returns:
        Settings instance
    """
    settings = Settings()
    settings.create_directories()
    return settings"""Test configuration and fixtures."""

import os
import uuid
from typing import Generator
import time

import pytest
import psycopg2
from sqlalchemy import create_engine, text
from sqlalchemy.orm import Session, sessionmaker

from local_newsifier.models.database import Base
from local_newsifier.config.database import DatabaseSettings


def get_test_db_name() -> str:
    """Get a unique test database name.
    
    Returns:
        A unique test database name based on process ID and timestamp
    """
    pid = os.getpid()
    timestamp = int(time.time())
    return f"test_local_newsifier_{pid}_{timestamp}"


@pytest.fixture(scope="session")
def postgres_url():
    """Get PostgreSQL URL for tests."""
    settings = DatabaseSettings()
    test_db_name = get_test_db_name()
    base_url = settings.get_database_url()
    return base_url.replace(settings.POSTGRES_DB, test_db_name)


@pytest.fixture(scope="session")
def test_engine(postgres_url):
    """Create a test database engine.
    
    This version is compatible with both local development and CI environments.
    """
    test_db_name = postgres_url.rsplit('/', 1)[1]
    
    # Connect to default postgres database to create test db
    try:
        # Connect to default postgres database to create test db
        admin_url = postgres_url.rsplit('/', 1)[0] + "/postgres"
        admin_engine = create_engine(admin_url, isolation_level="AUTOCOMMIT")
        
        # Create test database
        with admin_engine.connect() as conn:
            conn.execute(text(f"DROP DATABASE IF EXISTS {test_db_name}"))
            conn.execute(text(f"CREATE DATABASE {test_db_name}"))
    except Exception as e:
        print(f"Error creating test database: {e}")
        # If we can't create the database, try connecting directly
        # (it might already exist in CI)
    
    # Create engine for the test database
    engine = create_engine(postgres_url)
    
    # Create all tables
    Base.metadata.create_all(engine)
    
    yield engine
    
    # Cleanup after all tests
    Base.metadata.drop_all(engine)
    engine.dispose()  # Close all connections
    
    try:
        # Connect to default postgres database to drop test db
        admin_url = postgres_url.rsplit('/', 1)[0] + "/postgres"
        admin_engine = create_engine(admin_url, isolation_level="AUTOCOMMIT")
        
        # Drop test database
        with admin_engine.connect() as conn:
            conn.execute(text(f"""
                SELECT pg_terminate_backend(pg_stat_activity.pid)
                FROM pg_stat_activity
                WHERE pg_stat_activity.datname = '{test_db_name}'
                AND pid <> pg_backend_pid();
            """))
            conn.execute(text(f"DROP DATABASE IF EXISTS {test_db_name}"))
    except Exception as e:
        print(f"Error dropping test database: {e}")
        # In CI, we may not have permissions to drop databases


@pytest.fixture(autouse=True)
def setup_test_db(test_engine) -> Generator[None, None, None]:
    """Set up and tear down the test database for each test."""
    # Drop all tables and recreate schema
    with test_engine.connect() as conn:
        # Drop all tables
        conn.execute(text("""
            DO $$ 
            DECLARE
                r RECORD;
            BEGIN
                FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
                    EXECUTE 'DROP TABLE IF EXISTS ' || quote_ident(r.tablename) || ' CASCADE';
                END LOOP;
            END $$;
        """))
        conn.commit()
    
    # Create all tables
    Base.metadata.create_all(test_engine)
    yield
    
    # Drop all tables after tests
    with test_engine.connect() as conn:
        conn.execute(text("""
            DO $$ 
            DECLARE
                r RECORD;
            BEGIN
                FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
                    EXECUTE 'DROP TABLE IF EXISTS ' || quote_ident(r.tablename) || ' CASCADE';
                END LOOP;
            END $$;
        """))
        conn.commit()


@pytest.fixture
def db_session(test_engine) -> Generator[Session, None, None]:
    """Create a test database session."""
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=test_engine)
    session = SessionLocal()
    try:
        yield session
    finally:
        session.close() """Integration tests for the entity tracking database functionality."""

from datetime import UTC, datetime, timedelta
from typing import Generator

import pytest
from sqlalchemy.orm import Session

from local_newsifier.database.manager import DatabaseManager
from local_newsifier.models.database import ArticleCreate, ArticleDB, Base, EntityCreate, EntityDB
from local_newsifier.models.entity_tracking import (
    CanonicalEntity,
    CanonicalEntityCreate,
    CanonicalEntityDB,
    EntityMentionContext,
    EntityMentionContextCreate,
    EntityMentionContextDB,
    EntityProfile,
    EntityProfileCreate,
    EntityProfileDB,
    entity_mentions
)


@pytest.fixture
def db_manager(db_session: Session) -> DatabaseManager:
    """Create a database manager instance."""
    return DatabaseManager(db_session)


@pytest.fixture
def sample_article(db_manager: DatabaseManager):
    """Create a sample article."""
    article = ArticleCreate(
        url="https://example.com/biden",
        title="Article about Biden",
        content="Joe Biden is the president of the United States. "
                "He previously served as vice president under Barack Obama.",
        published_at=datetime.now(UTC),
        status="analyzed"
    )
    return db_manager.create_article(article)


@pytest.fixture
def sample_entity(db_manager: DatabaseManager, sample_article):
    """Create a sample entity."""
    entity = EntityCreate(
        article_id=sample_article.id,
        text="Joe Biden",
        entity_type="PERSON",
        confidence=0.95
    )
    return db_manager.add_entity(entity)


def test_create_canonical_entity(db_manager: DatabaseManager):
    """Test creating a canonical entity."""
    # Create canonical entity
    entity_data = CanonicalEntityCreate(
        name="Joe Biden",
        entity_type="PERSON",
        description="46th President of the United States"
    )
    
    canonical_entity = db_manager.create_canonical_entity(entity_data)
    
    # Verify entity was created
    assert canonical_entity.id is not None
    assert canonical_entity.name == "Joe Biden"
    assert canonical_entity.entity_type == "PERSON"
    assert canonical_entity.description == "46th President of the United States"
    assert canonical_entity.first_seen is not None
    assert canonical_entity.last_seen is not None


def test_get_canonical_entity(db_manager: DatabaseManager):
    """Test getting a canonical entity by ID."""
    # Create canonical entity
    entity_data = CanonicalEntityCreate(
        name="Kamala Harris",
        entity_type="PERSON",
        description="Vice President of the United States"
    )
    
    created_entity = db_manager.create_canonical_entity(entity_data)
    
    # Get canonical entity
    retrieved_entity = db_manager.get_canonical_entity(created_entity.id)
    
    # Verify entity was retrieved
    assert retrieved_entity is not None
    assert retrieved_entity.id == created_entity.id
    assert retrieved_entity.name == "Kamala Harris"
    assert retrieved_entity.entity_type == "PERSON"
    assert retrieved_entity.description == "Vice President of the United States"


def test_get_canonical_entity_by_name(db_manager: DatabaseManager):
    """Test getting a canonical entity by name and type."""
    # Create canonical entity
    entity_data = CanonicalEntityCreate(
        name="Barack Obama",
        entity_type="PERSON",
        description="44th President of the United States"
    )
    
    db_manager.create_canonical_entity(entity_data)
    
    # Get canonical entity by name
    retrieved_entity = db_manager.get_canonical_entity_by_name("Barack Obama", "PERSON")
    
    # Verify entity was retrieved
    assert retrieved_entity is not None
    assert retrieved_entity.name == "Barack Obama"
    assert retrieved_entity.entity_type == "PERSON"
    assert retrieved_entity.description == "44th President of the United States"


def test_add_entity_mention_context(db_manager: DatabaseManager, sample_entity):
    """Test adding context for an entity mention."""
    # Add entity mention context
    context_data = EntityMentionContextCreate(
        entity_id=sample_entity.id,
        article_id=sample_entity.article_id,
        context_text="Joe Biden is the president of the United States.",
        sentiment_score=0.5
    )
    
    context = db_manager.add_entity_mention_context(context_data)
    
    # Verify context was added
    assert context.id is not None
    assert context.entity_id == sample_entity.id
    assert context.article_id == sample_entity.article_id
    assert context.context_text == "Joe Biden is the president of the United States."
    assert context.sentiment_score == 0.5


def test_add_entity_profile(db_manager: DatabaseManager):
    """Test adding an entity profile."""
    # Create canonical entity
    entity_data = CanonicalEntityCreate(
        name="Donald Trump",
        entity_type="PERSON",
        description="45th President of the United States"
    )
    
    canonical_entity = db_manager.create_canonical_entity(entity_data)
    
    # Add entity profile
    profile_data = EntityProfileCreate(
        canonical_entity_id=canonical_entity.id,
        mention_count=10,
        contexts=["Donald Trump is a former president."],
        temporal_data={"2023-01-01": 5, "2023-01-02": 5}
    )
    
    profile = db_manager.add_entity_profile(profile_data)
    
    # Verify profile was added
    assert profile.id is not None
    assert profile.canonical_entity_id == canonical_entity.id
    assert profile.mention_count == 10
    assert profile.contexts is not None
    assert len(profile.contexts) == 1
    assert profile.contexts[0] == "Donald Trump is a former president."
    assert profile.temporal_data == {"2023-01-01": 5, "2023-01-02": 5}
    assert profile.last_updated is not None


def test_update_entity_profile(db_manager: DatabaseManager):
    """Test updating an entity profile."""
    # Create canonical entity
    entity_data = CanonicalEntityCreate(
        name="Joe Biden",
        entity_type="PERSON",
        description="46th President of the United States"
    )
    
    canonical_entity = db_manager.create_canonical_entity(entity_data)
    
    # Add initial profile
    profile_data = EntityProfileCreate(
        canonical_entity_id=canonical_entity.id,
        mention_count=5,
        contexts=["Joe Biden is the president."],
        temporal_data={"2023-01-01": 5}
    )
    
    initial_profile = db_manager.add_entity_profile(profile_data)
    
    # Update profile
    updated_profile_data = EntityProfileCreate(
        canonical_entity_id=canonical_entity.id,
        mention_count=10,
        contexts=["Joe Biden is the president.", "He was previously VP."],
        temporal_data={"2023-01-01": 5, "2023-01-02": 5}
    )
    
    updated_profile = db_manager.add_entity_profile(updated_profile_data)
    
    # Verify profile was updated
    assert updated_profile.id == initial_profile.id
    assert updated_profile.mention_count == 10
    assert updated_profile.contexts is not None
    assert len(updated_profile.contexts) == 2
    assert "Joe Biden is the president." in updated_profile.contexts
    assert "He was previously VP." in updated_profile.contexts
    assert updated_profile.temporal_data == {"2023-01-01": 5, "2023-01-02": 5}
    assert updated_profile.last_updated > initial_profile.last_updated


def test_entity_timeline_and_sentiment_trend(
    db_manager: DatabaseManager, db_session: Session
):
    """Test getting entity timeline and sentiment trend."""
    # Create article
    article = ArticleCreate(
        url="https://example.com/biden-timeline",
        title="Biden Timeline Article",
        content="Joe Biden announced new policies today.",
        published_at=datetime.now(UTC),
        status="analyzed"
    )
    
    created_article = db_manager.create_article(article)
    
    # Create canonical entity
    entity_data = CanonicalEntityCreate(
        name="Joe Biden",
        entity_type="PERSON"
    )
    
    canonical_entity = db_manager.create_canonical_entity(entity_data)
    
    # Create entity
    entity = EntityCreate(
        article_id=created_article.id,
        text="Joe Biden",
        entity_type="PERSON",
        confidence=0.95
    )
    
    created_entity = db_manager.add_entity(entity)
    
    # Add entity mention context
    context_data = EntityMentionContextCreate(
        entity_id=created_entity.id,
        article_id=created_article.id,
        context_text="Joe Biden announced new policies today.",
        sentiment_score=0.7
    )
    
    db_manager.add_entity_mention_context(context_data)
    
    # Create entity mention association
    db_session.execute(
        entity_mentions.insert().values(
            canonical_entity_id=canonical_entity.id,
            entity_id=created_entity.id,
            confidence=0.95
        )
    )
    db_session.commit()
    
    # Test get_entity_timeline
    start_date = datetime.now(UTC) - timedelta(days=7)
    end_date = datetime.now(UTC) + timedelta(days=1)
    
    timeline = db_manager.get_entity_timeline(
        canonical_entity.id, start_date, end_date
    )
    
    # Verify timeline
    assert len(timeline) == 1
    assert timeline[0]["context"] == "Joe Biden announced new policies today."
    assert timeline[0]["sentiment_score"] == 0.7
    assert timeline[0]["article"]["title"] == "Biden Timeline Article"
    
    # Test get_entity_sentiment_trend
    trend = db_manager.get_entity_sentiment_trend(
        canonical_entity.id, start_date, end_date
    )
    
    # Verify trend (may be empty if date grouping doesn't work in test environment)
    if trend:
        assert trend[0]["avg_sentiment"] == 0.7